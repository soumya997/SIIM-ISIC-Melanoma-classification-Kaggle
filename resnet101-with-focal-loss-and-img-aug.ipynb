{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "**This Notebook is by Soumyadip Sarkar and this version is simply to help him get going by using preprocessed data and fixing a few things.**\n",
    "\n",
    "It is not a complete solution and it still has issues that I commented out and I will leave it to him to work through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U pandas_bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import gc\n",
    "import os\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "# import tensorflow.keras.applications.ResNet101 as resnet101\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# import pandas_bokeh\n",
    "# from bokeh.models.widgets import DataTable, TableColumn\n",
    "# from bokeh.models import ColumnDataSource\n",
    "\n",
    "from plotly.offline import iplot\n",
    "import plotly as py\n",
    "import plotly.tools as tls\n",
    "import cufflinks as cf\n",
    "\n",
    "py.offline.init_notebook_mode(connected = True)\n",
    "cf.go_offline()\n",
    "cf.set_config_file(theme = 'solar')\n",
    "\n",
    "# pd.set_option('plotting.backend', 'pandas_bokeh')\n",
    "# pandas_bokeh.output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "EPOCH = 10\n",
    "BATCH_SIZE = 32 #increased for small images taking less memory\n",
    "IMG_SIZE = 224\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>sex</th>\n",
       "      <th>age_approx</th>\n",
       "      <th>anatom_site_general_challenge</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>benign_malignant</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_2637011</td>\n",
       "      <td>IP_7279968</td>\n",
       "      <td>male</td>\n",
       "      <td>45.0</td>\n",
       "      <td>head/neck</td>\n",
       "      <td>NaN</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_0015719</td>\n",
       "      <td>IP_3075186</td>\n",
       "      <td>female</td>\n",
       "      <td>45.0</td>\n",
       "      <td>upper extremity</td>\n",
       "      <td>NaN</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISIC_0052212</td>\n",
       "      <td>IP_2842074</td>\n",
       "      <td>female</td>\n",
       "      <td>50.0</td>\n",
       "      <td>lower extremity</td>\n",
       "      <td>nevus</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ISIC_0068279</td>\n",
       "      <td>IP_6890425</td>\n",
       "      <td>female</td>\n",
       "      <td>45.0</td>\n",
       "      <td>head/neck</td>\n",
       "      <td>NaN</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISIC_0074268</td>\n",
       "      <td>IP_8723313</td>\n",
       "      <td>female</td>\n",
       "      <td>55.0</td>\n",
       "      <td>upper extremity</td>\n",
       "      <td>NaN</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     image_name  patient_id     sex  age_approx anatom_site_general_challenge  \\\n",
       "0  ISIC_2637011  IP_7279968    male        45.0                     head/neck   \n",
       "1  ISIC_0015719  IP_3075186  female        45.0               upper extremity   \n",
       "2  ISIC_0052212  IP_2842074  female        50.0               lower extremity   \n",
       "3  ISIC_0068279  IP_6890425  female        45.0                     head/neck   \n",
       "4  ISIC_0074268  IP_8723313  female        55.0               upper extremity   \n",
       "\n",
       "  diagnosis benign_malignant  target  \n",
       "0       NaN           benign       0  \n",
       "1       NaN           benign       0  \n",
       "2     nevus           benign       0  \n",
       "3       NaN           benign       0  \n",
       "4       NaN           benign       0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('../input/siim-isic-melanoma-classification/train.csv',na_values=['unknown'])\n",
    "test = pd.read_csv('../input/siim-isic-melanoma-classification/test.csv')\n",
    "submission = pd.read_csv('../input/siim-isic-melanoma-classification/sample_submission.csv')\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DIR = '../input/siim-isic-melanoma-classification/jpeg/train/'\n",
    "DIR = '../input/siic-isic-224x224-images/train/'\n",
    "img = []\n",
    "labels = []\n",
    "#jpg = '.jpg'\n",
    "png = '.png'\n",
    "\n",
    "for i in train['image_name']:\n",
    "    img.append(os.path.join(DIR,i)+png)\n",
    "    \n",
    "for i in train['target']:\n",
    "    labels.append(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_val,y_train,y_val = train_test_split(img,labels,test_size = 0.2,random_state = SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "                                         rescale=1./255,\n",
    "                                        rotation_range=40,\n",
    "                                         horizontal_flip=True,\n",
    "                                         vertical_flip= True,\n",
    "                                        width_shift_range=0.2,\n",
    "                                        height_shift_range=0.2,\n",
    "                                        \n",
    ")\n",
    "\n",
    "val_data_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "                                            rescale=1./255\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../input/siic-isic-224x224-images/train/ISIC_6...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../input/siic-isic-224x224-images/train/ISIC_6...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../input/siic-isic-224x224-images/train/ISIC_8...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../input/siic-isic-224x224-images/train/ISIC_4...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../input/siic-isic-224x224-images/train/ISIC_0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image  target\n",
       "0  ../input/siic-isic-224x224-images/train/ISIC_6...       0\n",
       "1  ../input/siic-isic-224x224-images/train/ISIC_6...       0\n",
       "2  ../input/siic-isic-224x224-images/train/ISIC_8...       0\n",
       "3  ../input/siic-isic-224x224-images/train/ISIC_4...       0\n",
       "4  ../input/siic-isic-224x224-images/train/ISIC_0...       0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_img = pd.DataFrame(x_train,columns=['image'])\n",
    "train_labels = pd.DataFrame(y_train,columns=['target'])\n",
    "train_data = pd.concat([train_img,train_labels],axis = 1)\n",
    "\n",
    "val_img = pd.DataFrame(x_val,columns=['image'])\n",
    "val_labels = pd.DataFrame(y_val,columns=['target'])\n",
    "val_data = pd.concat([val_img,val_labels],axis = 1)\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26500 validated image filenames.\n",
      "Found 6626 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "train_img_gen = train_data_generator.flow_from_dataframe(train_data,\n",
    "    x_col='image',\n",
    "    y_col='target',\n",
    "    target_size=(IMG_SIZE,IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    class_mode='raw')\n",
    "\n",
    "val_img_gen = val_data_generator.flow_from_dataframe(val_data,\n",
    "                                                    x_col = 'image',\n",
    "                                                    y_col = 'target',\n",
    "                                                    target_size= (IMG_SIZE,IMG_SIZE),\n",
    "                                                    batch_size=BATCH_SIZE,\n",
    "                                                    shuffle=True,\n",
    "                                                    class_mode='raw')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # , because of class imbalance it's better to use focal loss rather than normal binary_crossentropy.You can read more about it here\n",
    "\n",
    "# def focal_loss(alpha=0.25,gamma=2.0):\n",
    "#     def focal_crossentropy(y_true, y_pred):\n",
    "#         bce = K.binary_crossentropy(y_true, y_pred)\n",
    "        \n",
    "#         y_pred = K.clip(y_pred, K.epsilon(), 1.- K.epsilon())\n",
    "#         p_t = (y_true*y_pred) + ((1-y_true)*(1-y_pred))\n",
    "        \n",
    "#         alpha_factor = 1\n",
    "#         modulating_factor = 1\n",
    "\n",
    "#         alpha_factor = y_true*alpha + ((1-alpha)*(1-y_true))\n",
    "#         modulating_factor = K.pow((1-p_t), gamma)\n",
    "\n",
    "#         # compute the final loss and return\n",
    "#         return K.mean(alpha_factor*modulating_factor*bce, axis=-1)\n",
    "#     return focal_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "def focal_loss(alpha=0.25,gamma=2.0):\n",
    "    def focal_crossentropy(y_true, y_pred):\n",
    "        bce = K.binary_crossentropy(y_true, y_pred)\n",
    "        \n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1.- K.epsilon())\n",
    "        p_t = (y_true*y_pred) + ((1-y_true)*(1-y_pred))\n",
    "        \n",
    "        alpha_factor = 1\n",
    "        modulating_factor = 1\n",
    "\n",
    "        alpha_factor = y_true*alpha + ((1-alpha)*(1-y_true))\n",
    "        modulating_factor = K.pow((1-p_t), gamma)\n",
    "\n",
    "        # compute the final loss and return\n",
    "        return K.mean(alpha_factor*modulating_factor*bce, axis=-1)\n",
    "    return focal_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet101_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "171450368/171446536 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(tf.keras.applications.ResNet101(weights='imagenet',\n",
    "                                        include_top=False,\n",
    "                                        input_shape=(IMG_SIZE,IMG_SIZE,3)\n",
    "                                       ))\n",
    "model.add(layers.GlobalAveragePooling2D())\n",
    "#model.add(layers.Flatten())\n",
    "#model.add(layers.BatchNormalization())\n",
    "#model.add(layers.Dense(1024,activation= 'relu'))\n",
    "#model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Dense(70000,activation= 'relu'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Dense(20000,activation= 'relu'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Dense(1000,activation= 'relu'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "#model.add(layers.Dense(256,activation= 'relu'))\n",
    "#model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(1,activation='sigmoid'))\n",
    "model.layers[0].trainable = True\n",
    "\n",
    "model.compile(loss=[focal_loss(alpha=0.25,gamma=2.0)],metrics=[tf.keras.metrics.AUC()],optimizer='adam' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet101 (Model)            (None, 7, 7, 2048)        42658176  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 42,660,225\n",
      "Trainable params: 42,554,881\n",
      "Non-trainable params: 105,344\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tf.keras.callbacks import ModelCheckpoint\n",
    "# filepath = \"../output/saved_models-improvement-{epoch:02d}-{auc:.2f}.hdf5\"\n",
    "# checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath,monitor = 'auc',verbose = 1,save_best_only = True,mode = 'max')\n",
    "# callbacks_list = [checkpoint]\n",
    "\n",
    "mc = tf.keras.callbacks.ModelCheckpoint('best_model.h5',monitor=tf.keras.metrics.AUC(),mode='min',save_best_only=True,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:71: FutureWarning:\n",
      "\n",
      "Pass classes=[0 1], y=0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "26495    0\n",
      "26496    0\n",
      "26497    0\n",
      "26498    0\n",
      "26499    0\n",
      "Name: target, Length: 26500, dtype: int64 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(train_data.target),\n",
    "                                                 train_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.50893029, 1: 28.49462366}\n"
     ]
    }
   ],
   "source": [
    "class_weight ={\n",
    "    0:0.50893029,\n",
    "    1:28.49462366\n",
    "}\n",
    "print(class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "828/828 [==============================] - 536s 648ms/step - loss: 0.1789 - auc: 0.5280 - val_loss: 7.4103 - val_auc: 0.3466\n",
      "Epoch 2/10\n",
      "828/828 [==============================] - 480s 580ms/step - loss: 0.1200 - auc: 0.6311 - val_loss: 0.0514 - val_auc: 0.7182\n",
      "Epoch 3/10\n",
      "828/828 [==============================] - 480s 579ms/step - loss: 0.0784 - auc: 0.7123 - val_loss: 2.9083 - val_auc: 0.6028\n",
      "Epoch 4/10\n",
      "828/828 [==============================] - 482s 582ms/step - loss: 0.0758 - auc: 0.7046 - val_loss: 0.0259 - val_auc: 0.7493\n",
      "Epoch 5/10\n",
      "828/828 [==============================] - 486s 587ms/step - loss: 0.0664 - auc: 0.7232 - val_loss: 0.0554 - val_auc: 0.7543\n",
      "Epoch 6/10\n",
      "828/828 [==============================] - 488s 590ms/step - loss: 0.0673 - auc: 0.7224 - val_loss: 0.0370 - val_auc: 0.7511\n",
      "Epoch 7/10\n",
      "828/828 [==============================] - 494s 596ms/step - loss: 0.1001 - auc: 0.6774 - val_loss: 0.0753 - val_auc: 0.5111\n",
      "Epoch 8/10\n",
      "828/828 [==============================] - 494s 597ms/step - loss: 0.0798 - auc: 0.6123 - val_loss: 0.0926 - val_auc: 0.6348\n",
      "Epoch 9/10\n",
      "828/828 [==============================] - 496s 599ms/step - loss: 0.0700 - auc: 0.6996 - val_loss: 0.0499 - val_auc: 0.7202\n",
      "Epoch 10/10\n",
      "828/828 [==============================] - 495s 598ms/step - loss: 0.0706 - auc: 0.7267 - val_loss: 0.0503 - val_auc: 0.6473\n"
     ]
    }
   ],
   "source": [
    "\n",
    "History = model.fit_generator(train_img_gen,\n",
    "                             steps_per_epoch=train_data.shape[0]//BATCH_SIZE,\n",
    "                             epochs=EPOCH,\n",
    "                             validation_data=val_img_gen,\n",
    "                             validation_steps=val_data.shape[0]//BATCH_SIZE,\n",
    "                             class_weight=class_weight,\n",
    "                             callbacks=[mc]\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.models.save_model(model,'model_resnet101.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_resnet101.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Here I useed the Resnet101 net work, with Image Aug. And as the data set is imbalanced so I used focal loss and weighted class approach."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Thank you for reading!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "resnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss, accu = model.evaluate(X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dir='/kaggle/input/siim-isic-melanoma-classification/jpeg/test/'\n",
    "# test_data=[]\n",
    "# for i in range(test.shape[0]):\n",
    "#     test_data.append(test_dir + test['image_name'].iloc[i]+'.jpg')\n",
    "# df_test=pd.DataFrame(test_data)\n",
    "# df_test.columns=['images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target=[]\n",
    "# for path in df_test['images']:\n",
    "#     img=cv2.imread(str(path))\n",
    "#     img = cv2.resize(img, (224,224))\n",
    "#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#     img = img.astype(np.float32)/255.\n",
    "#     img=np.reshape(img,(1,224,224,3))\n",
    "#     prediction=model.predict(img)\n",
    "#     target.append(prediction[0][0])\n",
    "\n",
    "# # submission['target']=target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission['target']=target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission.to_csv('submission.csv', index=False)\n",
    "# submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "i am a student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
